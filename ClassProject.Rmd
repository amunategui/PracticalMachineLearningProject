Practical Machine Learning Project
========================================================

Executive Summary

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project we use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The goal is to predict the manner in which they did the exercise. 
<BR><BR>
Getting the data online once and subsequently loading it from a local directory:
```{r}
setwd('/Volumes/NO NAME/classes/Hopkins/Practical Machine Learning/')
library(data.table)
library(RCurl)
library(caret)

# urlfile <-'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
# download.file(urlfile, destfile = "temp.csv", method = "curl")
# dfTrain <-read.csv('temp.csv')
# write.csv(dfTrain, 'train.csv',row.names=F)
dfTrain <-read.csv('train.csv')

# urlfile <-'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'
# download.file(urlfile, destfile = "temp.csv", method = "curl")
# dfTest <-read.csv('temp.csv')
# write.csv(dfTest, 'test.csv',row.names=F)
dfTest <-read.csv('test.csv')
```
<BR><BR>
Clean up the variables by removing problematic fields and imputating all NAs with either KNN or 0:
```{r}
# remove index, un-important features, and timestamps
dfTrain <- subset(dfTrain,select=-c(X, user_name, cvtd_timestamp, raw_timestamp_part_1, raw_timestamp_part_2))
# remove the outcome temporarily
outcomeData <- dfTrain$classe
dfTrain <- subset(dfTrain,select=-c(classe))
# get names of numeric data
originalNumericVariables <- names(dfTrain[sapply(dfTrain, is.numeric)])
# get names of factor data
originalFactorVariables <- names(dfTrain[sapply(dfTrain, is.factor)])
# binarize all numerical factors to numericals
dfFactors <- as.data.frame(lapply(dfTrain[c(originalFactorVariables)],as.numeric))
# impute numerics with nearest knn
dfNumerics <- dfTrain[originalNumericVariables]
imputedNums <- preProcess(dfNumerics, method=c('knnImpute'))
dfNumerics <- predict(imputedNums, dfNumerics)
# rebuild data frame
dfCleanTrain <- cbind(dfFactors,dfNumerics)
# remove near zero variance
zeroVar <- nearZeroVar(dfCleanTrain)
dfCleanTrain <- dfCleanTrain[-zeroVar]
# impute any other NAs with 0
dfCleanTrain[is.na(dfCleanTrain)] <- 0 
# add the classe variable back to the data set
dfCleanTrain$classe <- outcomeData 
```
<BR><BR>
Clean up testing data set 
```{r}
# clean up test data set
dfCleanTest <- dfTest[c(intersect(names(dfCleanTrain),names(dfTest)))]
# get names of numeric data
originalNumericVariables <- names(dfCleanTest [sapply(dfCleanTest , is.numeric)])
# get names of factor data
originalFactorVariables <- names(dfCleanTest [sapply(dfCleanTest , is.logical)])
# binarize all numerical factors to numericals
dfFactors <- as.data.frame(lapply(dfCleanTest [c(originalFactorVariables)],as.numeric))
# impute numerics with nearest knn
dfNumerics <- dfCleanTest[originalNumericVariables]
imputedNums <- preProcess(dfNumerics, method=c('knnImpute'))
dfNumerics <- predict(imputedNums, dfNumerics)
# rebuild data frame
dfCleanTest <- cbind(dfFactors,dfNumerics)
# impute any other NAs with 0
dfCleanTest[is.na(dfCleanTest)] <- 0 
# remove near zero variance
zeroVar <- nearZeroVar(dfCleanTest)
dfCleanTest <- dfCleanTest[-zeroVar]
```
<BR><BR>
Balance out both data sets to only use mutually inclusive features
```{r}
# balance both train and test datasets
predictors <- intersect(names(dfCleanTrain),names(dfCleanTest))
dfCleanTrain <- dfCleanTrain[c(predictors, 'classe')]
dfCleanTest <- dfCleanTest[c(predictors)]
```
<BR><BR>
Shuffle the data...
```{r}
# shuffle the whole data set
set.seed(1234)
dfCleanTrain <- dfCleanTrain[sample(nrow(dfCleanTrain)),]
```
<BR><BR>
With the training data set in a cleaned and ready state, we now model the entire data set with a <b>multinomial</b> model which can handle predicting multiple classes at a time, not just two.
```{recho=TRUE}
library(nnet)
model = multinom(classe~.,data=dfCleanTrain, maxit=1000, trace=F)
# Order top influential variables
topModels <- varImp(model)
topModels$Variables <- row.names(topModels)
topModels <- topModels[order(-topModels$Overall),]
topVariables <- head(topModels,120)$Variables
```
<BR><BR>
Here is the list of the top 10 variables ranked as most important by the varImp function:
```{recho=TRUE}
print(head(topVariables,10))
```
<BR><BR>
We now cross-validate the <b>multinomial</b> model using a different portion of the training data set as a test data set to get an more correct average error and accuracy score.
```{r}
f <- paste('classe ~ ',paste0(topVariables,collapse=' + '))
# cross validate cv times
cv <- 3
cvDivider <- floor(nrow(dfCleanTrain) / (cv+1))
indexCount <- 1

# use multinom model from nnet to predict all classes
# use metrics RMSLE function to averate teh root mean squared log error from all 
# runs
library(Metrics)
totalError <- c()
totalAccuracy <- c()
for (cv in seq(1:cv)) {
   # assign chunk to data test
   dataTestIndex <- c((cv * cvDivider):(cv * cvDivider + cvDivider))
   dataTest <- dfCleanTrain[dataTestIndex,]
   # everything else to train
   dataTrain <- dfCleanTrain[-dataTestIndex,]
   objModel <- multinom(as.formula(f),data=dataTrain, maxit=1000, trace=F)
   preds <- predict(objModel, type="class", newdata=dataTest)
   err <- ce(as.numeric(dataTest$classe), as.numeric(preds))
   totalError <- c(totalError, err)
   totalAccuracy <- c(totalAccuracy, postResample(dataTest$classe,preds)[[1]])
}
```
<BR><BR>
The accuracy of the model is found using postResample and returns:
```{r}
print(mean(totalAccuracy))
```
<BR><BR>
The error rate derived from the accuracy minus 1 (or directly from the metrics package function: ce):
```{r}
print(mean(totalError))
```
<BR><BR>
Finally we use the real test data set to predict the 20 cases using our model:
```{r}
f <- paste('classe ~ ',paste0(topVariables,collapse=' + '))
objModel <- multinom(as.formula(f), data=dfCleanTrain, maxit=1000)
preds <- predict(objModel, type="class", newdata=dfCleanTest)
preds
```